---
title: "End Joining Signatures"
author: "Charlie Soeder"
date: "3/5/2019"
output:
  pdf_document:
    toc: true
    toc_depth: 5
    number_sections: true
  html_document: default
bibliography: references.bib
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = '/Users/csoeder/Research/PSIseq/EJgrepper/')
knitr::opts_knit$set(root.dir = '/proj/cdjones_lab/csoeder/EJgrepper/')
#knitr::opts_knit$set(root.dir=peaDubDee)

library("yaml")
library("readr")
library("knitr")
library("ggbio")
library("VennDiagram")

library("tidyverse")

```


```{r include=FALSE}

human_readable_croncher <- function(num_in) {
	dig <- 3
	num_out <- formatC(num_in, digits=dig, format='g') %>% as.numeric() %>% sitools::f2si()
	return(num_out)
}

bam_summary_loader <- function(filename, aligner="bwa", reference='dm6'){
	
	tmp.df <- read_delim(filename, "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
	names(tmp.df) <- c("sample","measure","value")
	
	tmp.df$sample <- as.factor(tmp.df$sample)
	tmp.df$measure <- as.factor(tmp.df$measure)
	tmp.df$aligner <- as.factor(aligner)
	tmp.df$reference <- as.factor(reference)
	
	return(tmp.df)
	
}

```

# Introduction

[@McVey2008]
[@Miller2016]


# Materials, Methods, Data, Software

```{r include=FALSE}

trammel <- read_yaml("config.yaml")

```

## Reference Genomes



```{r include=FALSE}
refGenomes_summary_df <- read_delim("meta/reference_genomes.summary", 
    "\t", escape_double = FALSE, col_names = FALSE, 
    trim_ws = TRUE)

names(refGenomes_summary_df) <- c("refGenome","measure","value")

```


```{r echo=FALSE}

refGenomes_summary_df %>% mutate( value=human_readable_croncher(value)) %>% spread(refGenome, value) %>% rename('Reference Genome:'=measure)  %>% kable(caption="Size and Consolidation of Reference Genomes")

```


## Sequenced Reads





```{r include=FALSE}
data_sets.df <- plyr::ldply(trammel$data_sets, data.frame)
data_sets.df$name <- as.factor(data_sets.df$name)
data_sets.df$paired<- as.factor(data_sets.df$paired)
data_sets.df$experimental<- as.factor(data_sets.df$experimental)
data_sets.df$source<- as.factor(data_sets.df$source)
```


Control sequences were provided by Danny Miller.



```{r echo=FALSE, results='asis'}
data_sets.df %>% filter(subgroups=='all') %>% group_by(experimental) %>% summarise(sample_count=n()) %>% kable(caption="Number of Sequenced Samples by Treatment")
```

```{r echo=FALSE}
data_sets.df %>% filter(subgroups=='all') %>%  select(c(name, paired, experimental, source)) %>%  arrange(experimental, desc(name)) %>% kable(caption="Sequenced Experimental Samples")
```


### Pre-Processing

These reads were preprocessed with FASTP [@Chen2018] for quality control and analytics. 


```{r echo=FALSE, include=FALSE}
fastp_summary <- read_delim("meta/sequenced_reads.dat", "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
names(fastp_summary ) <- c("name","type","measure","value")
fastp_summary$name <- as.factor(fastp_summary$name)
fastp_summary$type <- as.factor(fastp_summary$type)
fastp_summary$measure <- as.factor(fastp_summary$measure)
```




```{r echo=FALSE, include=FALSE}
filtration_stats <- inner_join(fastp_summary %>%  filter(type=="prefiltered" | type == 'postfiltered'), data_sets.df, by=c("name"="name"))
filtration_stats$type <- factor(filtration_stats$type, levels=c("prefiltered", "postfiltered"))
```


Starting FASTQ files contained a total of  $`r sum( filtration_stats %>% filter(type =='prefiltered') %>%  filter(measure=='total_reads') %>% select(value) ) %>% human_readable_croncher() `$ reads; after QC, this dropped to $`r sum( filtration_stats %>% filter(type =='postfiltered') %>%  filter(measure=='total_reads') %>% select(value) ) %>% human_readable_croncher() `$. 



```{r echo=FALSE}
pre_post_counts <- filtration_stats %>% filter(measure=='total_reads') %>%  group_by(type)  %>%  summarise(minimum = min(value), average=mean(value) , maximum = max(value)) 
retention_percent <- filtration_stats %>% filter(measure=='total_reads') %>%  filter(subgroups=="all")%>% select(c(name,type,value)) %>%  spread(type,value) %>% mutate(retention=100*postfiltered/prefiltered) %>%  summarise(type='percent retention', minimum = min(retention), average=mean(retention) , maximum = max(retention))
```
```{r echo=FALSE}
rbind(pre_post_counts, retention_percent) %>% mutate(minimum = human_readable_croncher(minimum), average=human_readable_croncher(average) , maximum = human_readable_croncher(maximum)) %>% kable(caption="Read Count and Percent Retention")
```



Filtration also increased the read quality, as seen in the increase in the fraction of reads with an average quality score > 30:

```{r echo=FALSE}
ggplot(filtration_stats %>% filter(measure == "q30_rate")) + geom_line(aes(group=name, x=type,y=100*value)) +  geom_point(aes(x=type, y = 100*value, color=name)) + labs(title = "Percent of Reads with a mean QUAL > 30", y="Percent QUAL > 30", x="") + theme_clear()

```



Duplicate reads were also detected; these will be filtered during alignment:

```{r echo=FALSE, include=FALSE}
dupe_stats <- inner_join(fastp_summary %>% filter(type=='duplication' & measure =='rate') %>%  mutate(percent=value*100) %>% select(c(name,percent)), data_sets.df, by=c("name"="name"))
```

```{r echo=FALSE}
dupe_stats %>%  summarise(minimum = min(percent), average=mean(percent), median=median(percent) , maximum = max(percent)) %>% kable(caption="Percentage Duplication",digits=1)
```

```{r echo=FALSE}
ggplot(dupe_stats) + geom_histogram(aes(x=percent), bins=15) + labs(title="Duplication Histogram", x="Read Duplication Rate (FASTP estimate)", y="Number Samples") + theme_clear()
```



## Mapped Reads

Reads were first mapped to the reference genome using the BWA SAMPE/SE algorithm. Then, the alignment file was filtered for uniqueness (ie, a read must be aligned optimally with no alternative or runner-up hits, "XT:A:U.*X0:i:1.*X1:i:0"), mapping/sequencing quality ("-q 20 -F 0x0100 -F 0x0200 -F 0x0300 -F 0x04"), and deduplication. 



```{r echo=FALSE, include=FALSE}

vs_dm6.bwa <- bam_summary_loader(filename = "meta/alignments.vs_dm6.bwa.summary",aligner="bwa", reference="dm6")
vs_dm6.bwaUniq <- bam_summary_loader(filename = "meta/alignments.vs_dm6.bwaUniq.summary",aligner="bwaUniq", reference="dm6")

all_alignments <- rbind(vs_dm6.bwa, vs_dm6.bwaUniq)
```



### Read & Alignment Quality



```{r echo=FALSE}
readcount_process <- all_alignments %>%  filter( (measure=='total_read_count' & aligner=="bwa") | measure == 'total_mapped_count' ) %>% mutate(measure=ifelse(aligner=="bwaUniq", "filtered_mapped_count", ifelse(measure=="total_read_count","total_read_count","total_mapped_count"))) 
readcount_process$measure <- factor(readcount_process$measure, levels = c('total_read_count','total_mapped_count','filtered_mapped_count'))
```

The fraction of reads retained at each filtration step:


```{r echo=FALSE}
ggplot(readcount_process) + geom_line(aes(group=sample, x=measure,y=value)) + geom_point(aes(x=measure, y=value, group=sample,color=sample))  + labs(title="Read Counts by Processing Step: Unmapped, Mapped, Filtered", x="", y="Number Reads" ) + theme_clear() + theme(axis.text.x = element_text(angle = -45, hjust = 0), legend.position='none' ) 

```



```{r echo=FALSE}

readcount_process.spread <- readcount_process %>% select(-c(aligner)) %>%  spread(measure, value) %>%  mutate(mapping_retention=total_mapped_count/total_read_count, filter_retention = filtered_mapped_count/total_mapped_count)


readcount_process.spread%>% gather(total_read_count:filtered_mapped_count, key="measure", value="value") %>%  group_by(measure) %>% summarise(minimum = min(value), average=mean(value), median = median(value), maximum = max(value)) %>% mutate(minimum = human_readable_croncher(minimum), average=human_readable_croncher(average), median = human_readable_croncher(median), maximum = human_readable_croncher(maximum)) %>% kable(caption="Read Counts During Alignment & Filtration")

```


```{r echo=FALSE}
readcount_process.spread %>% gather(mapping_retention:filter_retention, key="measure", value="value") %>%  group_by(measure) %>% summarise(minimum = min(100*value), average=mean(100*value), median = median(100*value), maximum = max(100*value)) %>% mutate(minimum = human_readable_croncher(minimum), average=human_readable_croncher(average), median = human_readable_croncher(median), maximum = human_readable_croncher(maximum)) %>%  kable(caption="Percentage of Reads Retained at Each Step",digits=1)

```



### Depth & Breadth of Coverage


```{r echo=FALSE, include=FALSE}

before_After.cov <- inner_join( vs_dm6.bwa %>%  filter(measure=='avg_depth' | measure == 'total_breadth') %>% select(-c(aligner)) %>% spread(measure, value), vs_dm6.bwaUniq %>%  filter(measure=='avg_depth' | measure == 'total_breadth') %>% select(-c(aligner)) %>% spread(measure, value), by='sample', suffix=c(".before",".after") ) %>%  mutate(depth_retention = avg_depth.after/avg_depth.before, breadth_retention=total_breadth.after/total_breadth.before)


before_After.cov.gathered.meta <- inner_join(before_After.cov %>%  gather(avg_depth.before:breadth_retention, key="measure", value="value") , data_sets.df, by=c("sample"="name") ) 


```


Depth of coverage, ie, the genome-wide average number of mapped reads per base pair:


```{r echo=FALSE}

depth.process <- all_alignments  %>%  filter(measure=='avg_depth' )%>% spread(aligner, value) %>%  mutate(depth_retention = 100*bwaUniq/bwa) %>% rename( "before" = bwa,  "after" = bwaUniq)

covstats.dpth <- depth.process %>% summarise(step="pre-filtration depth",minimum = min(before), average=mean(before), median = median(before), maximum = max(before))

covstats.dpth <- rbind(covstats.dpth, depth.process %>% summarise(step="post-filtration depth",minimum = min(after), average=mean(after), median = median(after), maximum = max(after)))

covstats.dpth <- rbind(covstats.dpth, depth.process  %>% summarise(step="depth retention percent",minimum = min(depth_retention), average=mean(depth_retention), median = median(depth_retention), maximum = max(depth_retention)))

covstats.dpth %>% kable(caption="Depth of Coverage Statistics for Raw and Filtered Alignments", digits=1 )
```


```{r echo=FALSE}
ggplot(depth.process %>%  select(-c(depth_retention,measure)) %>%  gather(before:after, key=measure, value=value) %>% mutate(measure=factor(measure, levels=c("before","after"))))  +geom_line(aes(group=sample, x=measure,y=value))+ geom_point(aes(group=sample, x=measure,y=value, color=sample)) + geom_text(data=depth.process %>%  select(-c(depth_retention,measure)) %>%  gather(before:after, key=measure, value=value) %>% mutate(measure=factor(measure, levels=c("before","after"))) %>% filter(value < 25), aes(as.factor("after"),value,label=sample)) + theme_clear()  + theme( legend.position='none' ) +  labs(title="Depth Of Coverage for Raw and Filtered Alignments", x="", y="Reads Per BP, Genome-Wide" ) 

```


```{r echo=FALSE, include=FALSE}
breadth.process <- all_alignments %>%  filter(measure=='total_breadth' ) %>% spread(aligner, value) %>%  mutate(breadth_retention = 100*bwaUniq/bwa) %>% rename( "before" = bwa, "after" = bwaUniq)

covstats.brdth <- breadth.process %>% summarise(step="pre-filtration breadth",minimum = 100*min(before), average=100*mean(before), median = 100*median(before), maximum = 100*max(before))

covstats.brdth <- rbind(covstats.brdth, breadth.process %>% summarise(step="post-filtration breadth",minimum = 100*min(after), average=100*mean(after), median = 100*median(after), maximum = 100*max(after)))

covstats.brdth <- rbind(covstats.brdth, breadth.process  %>% summarise(step="breadth retention percent",minimum = min(breadth_retention), average=mean(breadth_retention), median = median(breadth_retention), maximum = max(breadth_retention)))

covstats.brdth %>% kable(caption="Breadth of Coverage Statistics for Raw and Filtered Alignments", digits=1 )

```


Breadth of coverage, ie, the percentage of the genome covered by at least one read:

```{r include=FALSE}
ggplot(breadth.process %>%  select(-c(breadth_retention, measure)) %>%  gather(before:after, key=measure, value=value) %>% mutate(measure=factor(measure, levels=c("before","after")), value=100*value) ) + geom_point(aes(group=sample, x=measure,y=value, color=sample)) + geom_line(aes(group=sample, x=measure,y=value)) + geom_text(data=breadth.process %>%  select(-c(breadth_retention, measure)) %>%  gather(before:after, key=measure, value=value) %>% mutate(measure=factor(measure, levels=c("before","after")), value=100*value) %>% filter(value < 87), aes(as.factor("after"),value,label=sample)) + theme_clear()  + theme( legend.position='none' ) + labs(title="Breadth Of Coverage for Raw and Filtered Alignments", x="", y="Percentage of Reference Genome Mapped To" )

```


## Called Variants

The BWA and BWA-Uniq alignments were independently used to call variants:

mappings were used to jointly call variants in VCF format via Freebayes [@Garrison2012] using standard filters. 



```{r echo=FALSE, include=FALSE}
all_samples.vs_dm6.calledVariants.summary <- read_delim("meta/all_samples.vs_dm6.calledVariants.summary", "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
names(all_samples.vs_dm6.calledVariants.summary) <- c("subset", "aligner", "variant_type", "count")
all_samples.vs_dm6.calledVariants.summary$aligner <- as.factor(all_samples.vs_dm6.calledVariants.summary$aligner)
all_samples.vs_dm6.calledVariants.summary$variant_type <- as.factor(all_samples.vs_dm6.calledVariants.summary$variant_type)


reference_genomes_summaryStats.sprud <- refGenomes_summary_df %>%  spread(measure,value) %>%  select(c(-number_contigs)) 


all_samples.vs_dm6.calledVariants.summary.sprud <- all_samples.vs_dm6.calledVariants.summary %>% spread(variant_type, count) %>%  transmute( aligner=aligner, INDELs = total_indel_count, SNPs = total_snp_count) 

all_samples.vs_dm6.calledVariants.summary.sprud$total_bp <- reference_genomes_summaryStats.sprud$number_bases


all_samples.vs_dm6.calledVariants.summary.sprud %>%  transmute( aligner=aligner, INDELs = INDELs, INDEL_per_kb=1000*INDELs/total_bp, SNPs = SNPs, SNP_per_kb=1000*SNPs/total_bp) %>% mutate(INDELs=human_readable_croncher(SNPs), INDELs=human_readable_croncher(SNPs) ) %>%  kable(caption="Breadth of Coverage Statistics for Raw and Filtered Alignments", digits=1 )
```


To build this VCF, `r nrow(data_sets.df %>%  filter(subgroups == 'all'))` samples called jointly. However, not all sites were called in all samples (eg, due to coverage differences). The sites had the following group-wide call rate:



```{r echo=FALSE, include=FALSE}
all_samples.calledVariants.bwa.lmiss <- read_delim("meta/VCFs/all_samples.vs_dm6.bwa.summary.lmiss", "\t", escape_double = FALSE, trim_ws = TRUE)
all_samples.calledVariants.bwa.lmiss$aligner <- "bwa"

all_samples.calledVariants.bwaUniq.lmiss <- read_delim("meta/VCFs/all_samples.vs_dm6.bwaUniq.summary.lmiss", "\t", escape_double = FALSE, trim_ws = TRUE)
all_samples.calledVariants.bwaUniq.lmiss$aligner <- "bwaUniq"


nsamps <- nrow(data_sets.df %>%  filter(subgroups == 'all') )


all_samples.calledVariants.lmiss <- rbind(all_samples.calledVariants.bwa.lmiss, all_samples.calledVariants.bwaUniq.lmiss) %>% select(c(aligner, N_MISS)) %>%  mutate(aligner=as.factor(aligner), N_PRES=nsamps-N_MISS)

```



```{r echo=FALSE}
ggplot(all_samples.calledVariants.lmiss) + geom_freqpoly(aes(x=N_PRES, group=aligner, color=aligner), bins=nsamps) + scale_x_continuous(name ="Number Samples",limits=c(1,nsamps), breaks =seq(1,nsamps,1)) + theme_clear() + labs(title="Histogram of Variant Site Count,\n by Number of Samples Called At Site", y="Number of Sites")
```


The fraction of jointly called SNPs which are individually callable:




```{r echo=FALSE, include=FALSE}
all_samples.bwa.imiss <- read_delim("meta/VCFs/all_samples.vs_dm6.bwa.summary.imiss", "\t", escape_double = FALSE, trim_ws = TRUE)
all_samples.bwa.imiss$aligner <- "bwa"

all_samples.bwaUniq.imiss <- read_delim("meta/VCFs/all_samples.vs_dm6.bwaUniq.summary.imiss", "\t", escape_double = FALSE, trim_ws = TRUE)
all_samples.bwaUniq.imiss$aligner <- "bwaUniq"

all_samples.imiss  <- rbind(all_samples.bwa.imiss, all_samples.bwaUniq.imiss)  %>%  mutate(name=as.factor(INDV), aligner=as.factor(aligner), N_PRES=N_DATA-N_MISS) %>% select(c( name, N_MISS, N_PRES, F_MISS,aligner))

```

```{r echo=FALSE, include=FALSE}


all_samples.imiss.augmented <- inner_join(all_samples.imiss, all_alignments %>%  filter(measure=='total_breadth') %>% select(c(sample,aligner,value)) %>% rename(breadth=value) %>% mutate(breadth = 100*breadth), by=c("name"="sample", "aligner"="aligner"))

all_samples.imiss.augmented <-inner_join(all_samples.imiss.augmented, all_alignments %>%  filter(measure=='avg_depth') %>% select(c(sample,aligner,value)) %>% rename(depth=value), by=c("name"="sample", "aligner"="aligner"))

all_samples.imiss.augmented <- all_samples.imiss.augmented %>%  gather(breadth:depth, key="measure", value="value")

```

```{r echo=FALSE}
ggplot(all_samples.imiss.augmented) + geom_point(aes(x= value, y=1-F_MISS, color=aligner, shape=aligner)) + facet_grid(.~measure, scales="free_x") + theme_clear() + labs(x="", y="Fraction of Sites Callable", title="Jointly Called Variants Callable per Sample,\n by Breadth and Depth of Coverage")

```

### VCF comparison & contrast


The two variant-calling methods are compared and contrasted over the main-line chromosomes ( chr2L, chr2R, chr3L, chr3R, chr4, chrM, chrX, chrY). 
 

```{r echo=FALSE, include = FALSE}
vcfCompare_diff_both <- read_table2("meta/VCFs/all_samples__bwa.contrast.all_samples__bwaUniq.dm6.diff.sites.B", col_names = FALSE)
names(vcfCompare_diff_both) <- c("count","chrom")
vcfCompare_diff_both$vcf <- as.factor("both")

vcfCompare_diff_bwa <- read_table2("meta/VCFs/all_samples__bwa.contrast.all_samples__bwaUniq.dm6.diff.sites.1", col_names = FALSE)
names(vcfCompare_diff_bwa) <- c("count","chrom")
vcfCompare_diff_bwa$vcf <- as.factor("bwa")


vcfCompare_diff_bwaUniq <- read_table2("meta/VCFs/all_samples__bwa.contrast.all_samples__bwaUniq.dm6.diff.sites.2", col_names = FALSE)
names(vcfCompare_diff_bwaUniq) <- c("count","chrom")
vcfCompare_diff_bwaUniq$vcf <- as.factor("bwaUniq")

vcfCompare_diff <- rbind(vcfCompare_diff_both, vcfCompare_diff_bwa, vcfCompare_diff_bwaUniq)

```

Of the  $`r human_readable_croncher( vcfCompare_diff %>% filter(chrom=="total") %>%  select(count) %>% sum() ) `$ total variant sites called between the two VCFs, most ($`r human_readable_croncher( vcfCompare_diff %>% filter(chrom=="total" ) %>% filter(vcf=="both") %>%  select(count) %>% sum() ) `$) were called in both files; $`r human_readable_croncher( vcfCompare_diff %>% filter(chrom=="total" ) %>% filter(vcf!="both") %>%  select(count) %>% sum() ) `$ were unique to one of them:

```{r echo=FALSE}

grid.newpage()

draw.pairwise.venn(area1=sum(vcfCompare_diff %>% filter(vcf=='bwa' | vcf == "both") %>% filter(chrom=="total") %>%  select(count)), area2=sum(vcfCompare_diff %>% filter(vcf=='bwaUniq' | vcf == "both") %>% filter(chrom=="total") %>%  select(count)), cross.area =sum(vcfCompare_diff %>% filter(vcf == "both") %>% filter(chrom=="total") %>%  select(count)), category=c('bwa', 'bwaUniq'), fill = c("blue", "red"), alpha = rep(0.75, 2))

```



```{r echo=FALSE, include=FALSE}
vcfCompare_diff_discord.count <- read_table2("meta/VCFs/all_samples__bwa.contrast.all_samples__bwaUniq.dm6.disc.count", col_names = FALSE)
names(vcfCompare_diff_discord.count) <- c("count","chrom")
vcfCompare_diff_discord.count$vcf <- as.factor("discord")

```

Of the $`r human_readable_croncher( vcfCompare_diff %>% filter(chrom=="total" ) %>% filter(vcf=="both") %>%  select(count) %>% sum() ) `$ variable sites shared by the two VCF files,  $`r human_readable_croncher( vcfCompare_diff_discord.count %>% filter(chrom=="total" )  %>%  select(count) %>% sum()) `$ contained at least once sample which was called differently between the two VCF files. 




```{r echo=FALSE, include=FALSE}

vcfCompare_diff_discord <- read_delim("meta/VCFs/all_samples__bwa.contrast.all_samples__bwaUniq.dm6.disc", "\t", escape_double = FALSE, col_names = FALSE,trim_ws = TRUE)
names(vcfCompare_diff_discord) <- c("chrom", "pos", "vcf", "matching", "called", "discord", "disc_rate")
```



```{r echo=FALSE}
ggplot(vcfCompare_diff_discord) + geom_histogram(aes(x=disc_rate)) + theme_clear() + labs(x="Fraction Discordant Samples", y="Number of Variable Sites", title="Histogram of Variable Site Count\nby Fraction of Discordant Samples")

```


Combining disjoint and discordant variable sites, there were a total $`r human_readable_croncher( vcfCompare_diff_discord.count %>% filter(chrom=="total" )  %>%  select(count) %>% sum() + vcfCompare_diff %>% filter(chrom=="total" ) %>% filter(vcf!="both") %>%  select(count) %>% sum() ) `$ out of $`r human_readable_croncher( vcfCompare_diff %>% filter(chrom=="total") %>%  select(count) %>% sum() ) `$  disagreements between the two calling methods. 


# Results


# Bibliography




